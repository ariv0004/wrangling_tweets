{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">FIT5196 Assignment 1 (Task 1)</p>\n",
    "\n",
    "### Student Name : Audi Rivai\n",
    "### Student ID       : 30399262\n",
    "\n",
    "Last Updated: 13 August 2020\n",
    "\n",
    "Environment: Python 3.7.0 and Jupyter notebook Libraries used\n",
    "* re (for the regular expression, included in Anaconda Python 3.7)\n",
    "* os \n",
    "* langid (to classify the language)\n",
    "* BeautifulShop and xmltodict (for open the xml file)\n",
    "\n",
    "The content of this assignment are:\n",
    "1. Introduction\n",
    "2. Import Libraries\n",
    "3. Loading Data\n",
    "4. Parsing Data\n",
    "5. Find English Text\n",
    "6. Export Data\n",
    "7. Summary\n",
    "\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This FIT5196 Assignment extracts data from text format file named 30399262 that contain the date of information about Twitter respond and converts it into a readable file format (XML) which are 30399262.xml consecutively. The required tasks are the following:\n",
    "\n",
    "1. Extract the raw text from a `.txt` file by File I/O and regular expression then store into variable.\n",
    "2. Process the raw data by using Python Programming Language and several languages. \n",
    "3. Export the result into `.xml` and files.\n",
    "\n",
    "More details for each task will be given in the following cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "In this assignment, there are 3 libraries will be used to support the process. They are Re, os, and langid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from langid) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langid\n",
    "import re\n",
    "import os\n",
    "import langid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Data\n",
    "In this part, the raw text file named `30399262` folder using os package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2411 text file\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "# create a list of text file name\n",
    "all_text_files = os.listdir(\"30399262/\") # use os.listdir to open the directory inside the 30399262\n",
    "\n",
    "print(\"There are\", len(all_text_files), \"text file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parsing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section, the regular expression below will be used in order to capture the necessary word from the tasks\n",
    "\n",
    "__Part 1__ : Capture id number in each tweets.\n",
    "```python\n",
    "r'\"id\":\"(.*?)\"'\n",
    "```\n",
    "\n",
    "\n",
    "The explanation of the Regular Expression:\n",
    "- matches the characters \"id\":\" literally (case sensitive)\n",
    "- 1st Capturing Group (.*?)\n",
    "    - .*? matches any character (except for line terminators)\n",
    "    - *? Quantifier Matches between zero and unlimited times, as few times as possible, expanding as needed\n",
    "- \" matches the character \" literally (case sensitive)\n",
    "\n",
    "__Part 2__ : clean the id number if there are some string or punctuation.\n",
    "\n",
    "```python\n",
    "r'[^\\w\\s]'\n",
    "```\n",
    "\n",
    "\n",
    "The explanation of the Regular Expression:\n",
    "- Match a single character not present in the list below\n",
    "- \\w matches any word character (equal to [a-zA-Z0-9_])\n",
    "- \\s matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n",
    "\n",
    "__Part 3__ : Capture text in each tweets.\n",
    "\n",
    "\n",
    "```python\n",
    "r'\"text\":\"(.*?)(\"},\"|\",\"|\"},{)'\n",
    "```\n",
    "\n",
    "\n",
    "The explanation of the Regular Expression:\n",
    "- \"text\":\" matches the characters \"text\":\" literally (case sensitive)\n",
    "- 1st Capturing Group (.*?) list below:\n",
    "    - .*? matches any character (except for line terminators)\n",
    "    - , is matches the character , literally\n",
    "- 2nd Capturing Group (\"},\"|\",\"|\"},{):\n",
    "    - \"},\" matches the characters \"},\" literally (case sensitive)\n",
    "    - \",\" matches the characters \",\" literally (case sensitive)\n",
    "    - \"},{ matches the characters \"},{ literally (case sensitive)\n",
    "\n",
    "__Part 4__ : Capture date tweet in each tweets.\n",
    "    \n",
    "```python\n",
    "r'\"created_at\":\"(.*?)T'\n",
    "```\n",
    "\n",
    "\n",
    "The explanation of the Regular Expression:\n",
    "- since all string date ends with T\n",
    "- \"created_at\":\" matches the characters \"created_at\":\" literally (case sensitive)\n",
    "- 1st Capturing Group (.*?)\n",
    "    - .*? matches any character (except for line terminators)\n",
    "    - *? Quantifier â€” Matches between zero and unlimited times, as few times as possible, expanding as needed\n",
    "- T matches the character T literally (case sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213134\n",
      "213134\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create an empty list for each information that we want such as text data is for all the text that have been open\n",
    "data_list = []\n",
    "# empty file for id list\n",
    "id_list = []\n",
    "# empty file for date list\n",
    "date_list = []\n",
    "# empty file for twitter list\n",
    "twitter_list = []\n",
    "\n",
    "# create loop for each text data in file that need to be open and append the text into the data_list\n",
    "for data in range(len(all_text_files)):\n",
    "    file_open = '30399262/'+all_text_files[data]\n",
    "    with open(file_open,'r', encoding='utf-8') as infile:\n",
    "        text = infile.read()\n",
    "        data_list.append(text)\n",
    "    \n",
    "# apply the regular expression from part 1 and 2 for id number\n",
    "    pattern_id = re.compile(r'\"id\":\"(.*?)\"') # create a pattern \n",
    "    matches = re.finditer(pattern_id, data_list[data]) # find the matching pattern from the data_list\n",
    "# extract the id number from group 1 then delete some string such letter and punctuation\n",
    "    for m in matches:\n",
    "# then append the result to the id_list\n",
    "        id_list.append(re.sub(r'[^\\w\\s]', '', m.group(1)))\n",
    "    \n",
    "# apply the regular expression from part 3 and 4 for twitter text\n",
    "    pattern_tweets = re.compile(r'\"text\":\"(.*?)(\"},\"|\",\"|\"},{)') # create a pattern\n",
    "    for match in re.finditer(pattern_tweets, data_list[data]): # find the matching pattern from the data_list\n",
    "        twitter_text = (match.group(1))\n",
    "# append the result to twitter_list\n",
    "        twitter_list.append(twitter_text)\n",
    "# apply the regular expression from part 5 for date \n",
    "    pattern = re.compile(r'\"created_at\":\"(.*?)T') # create a pattern\n",
    "    for match in re.finditer(pattern, data_list[data]): # find the matching pattern from the data_list\n",
    "# extract the date from group 1 and add some \"\n",
    "        date = (match.group(1))\n",
    "# append the date to the date list\n",
    "        date_list.append('\"'+date+'\"')\n",
    "    \n",
    "print(len(twitter_list))\n",
    "print(len(id_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Find English Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will be looking which text is written in english and remove other languages using [function](https://github.com/saffsd/langid.py) `langid.classify()` we will be looking the result that contain english sentence. we will looping the twitter text and add into and empty list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets:  120415\n",
      "Number of Tweets:  120415\n",
      "Wall time: 8min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set time, it may take a while\n",
    "\n",
    "# create empty list for id, twitter text and date\n",
    "en_twitter_list = []\n",
    "en_id_list = []\n",
    "en_date = []\n",
    "\n",
    "# run a loop that get the englist ID twitter text and date using langid.classify\n",
    "for i in range(len(twitter_list)):\n",
    "    text = twitter_list[i]\n",
    "    # encode and decode the text into a utf-16 using eval\n",
    "    if \"\\\\u\" in text:\n",
    "        cleaner_text = eval(\"text.encode('utf_8').decode('unicode_escape')\")\\\n",
    "                        .encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "    else:\n",
    "        cleaner_text = text\n",
    "    # find the english fucntion\n",
    "    if langid.classify(cleaner_text)[0] == \"en\":\n",
    "        en_twitter_list.append(twitter_list[i])\n",
    "        en_id_list.append(id_list[i])\n",
    "        en_date.append(date_list[i])\n",
    "        \n",
    "print(\"Number of Tweets: \", len(en_twitter_list))\n",
    "print(\"Number of Tweets: \", len(en_id_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we know that there are plenty of tweets we need to clean the just to make sure the word is prepare to write in xml. Some words still contain with double qoutes [HTML](https://www.rapidtables.com/web/html/html-codes/htm-code-quote.html) decode. In this case we will be looking `&quot;` in html this is a \". Therefore we need to change them into on quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "# clean all the double backslash by using replace\n",
    "for text in range(len(en_twitter_list)):\n",
    "    # in the html single quotes in html is &quot; therefore we may replace this\n",
    "    new_text = en_twitter_list[text].replace(\"\\\\n\", \"\\n\")\\\n",
    "                .replace('\\\\\"', '&quot;').replace(\"\\\\r\", \"\")\n",
    "    clean_text.append(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will be put all the value into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for each date\n",
    "\n",
    "dct = list(zip(en_id_list, clean_text))\n",
    "dcti = {}\n",
    "for i, j in zip(en_date, dct):\n",
    "    dcti.setdefault(i, []).append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we aleady fill all he value into the dictioary, in this section we will be export the data into the xml file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the xml file that contain from the dictionary\n",
    "xml= open(\"30399262.xml\",\"w+\", encoding=\"utf-8-sig\")\n",
    "xml.write(\"<\"+'?xml version=\"1.0\" encoding=\"UTF-8\"?'+\">\\n\")\n",
    "xml.write(\"<\"+'data'+\">\\n\")\n",
    "for tweet_dates, value in dcti.items():\n",
    "    xml.write(\"<\"+'tweets date='+tweet_dates+\">\\n\")\n",
    "    for i in value:\n",
    "        tweets = i[1]\n",
    "        # use surrogatepass to print the emoji when they found an unicode\n",
    "        if \"\\\\u\" in tweets: \n",
    "            clean_tweets = eval(\"tweets.encode('utf_8').decode('unicode_escape')\")\\\n",
    "            .encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "        else:\n",
    "            clean_tweets = tweets\n",
    "        # clear the remain backslash\n",
    "        clean_tweets = clean_tweets.replace(\"\\\\\", \"\")\n",
    "        xml.write(\"<\"+'tweet id='+'\"'+i[0]+'\"'+\">\"+clean_tweets+\"</\"+\"tweet\"+\">\\n\")\n",
    "    xml.write(\"</\"+'tweets'+\">\\n\")\n",
    "xml.write(\"</\"+'data'+\">\")\n",
    "xml.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure that the xml file can be open we will be using 2 packages which is xmltodict and BeautifulSoup. The tools is to make sure that the file can be open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('30399262.xml', encoding=\"UTF-8\") as fd:\n",
    "#     doc = xmltodict.parse(fd.read())\n",
    "\n",
    "# doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the xml file can be seen perfectly. next we will be looking the xml using BeautifulSoup. Before we start using the BeautifulSoup there are some step that need to be done. since the xml data is large there is a configuration that need to be change in the jupyter notebook:\n",
    "* Open the anaconda command prompt\n",
    "* Type this codes:\n",
    "    * `jupyter notebook --generate-config`\n",
    "    * `jupyter notebook --NotebookApp.iopub_data_rate_limit=10e10`\n",
    "\n",
    "This code will reopen a new jupyter notebook that can open big memory file. After that you need to run the cells all over again and then start with the BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the xml file\n",
    "# soup = open('30399262.xml', \"r\", encoding=\"utf-8\")\n",
    "# content = soup.read()\n",
    "# soup = BeautifulSoup(content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find the data\n",
    "# soup.find('tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is help to understand of parsing the raw text file such as txt into readable file formats such as xml, json, etc. The regular expression is used to extract the text file. \n",
    "- **Tokenization, collocation extraction**. By using the `re` package, capturing text from text in order to make readable. \n",
    "- **Parsing**, Python Programming Language will be used for processing/parsing the raw text. \n",
    "- **Exporting data into specific format**. Basic Python File I/O is used to export the result into XML File Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "- Regex101. (2020, August 26). *Regular Expression 101* Retrieved from https://regex101.com/\n",
    "- MattDMo. (2016, July 1). *How to work with surrogate pairs in Python?*. Retrieved from http://stackoverflow.com/a/4739636\n",
    "- RapidTables. (2020, August 23) *HTML code for quotes*. Retrieved from https://www.rapidtables.com/web/html/html-codes/htm-code-quote.html\n",
    "- saffsd. (2017, July 15) *langid.py readme*. Retrieved from https://github.com/saffsd/langid.py\n",
    "- Python Software Foundation. (2020, August 23) *Regular expression operations* https://docs.python.org/2/library/re.html\n",
    "- Dan. (2010, February 6) *Counting the Number of keywords in a dictionary in python* https://stackoverflow.com/questions/2212433/counting-the-number-of-keywords-in-a-dictionary-in-python#:~:text=The%20number%20of%20distinct%20words,using%20the%20len()%20function.&text=To%20get%20all%20the%20distinct,keys()%20method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
